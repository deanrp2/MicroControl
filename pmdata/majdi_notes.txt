I will just write some notes here in case you want to reference them again

I included information on drum numbering, quadrant numbering and angular coordiate system
in ../README.md.

In utils.py I included 2 helpful functions to make life easier for you.
1. qpower_preprocess : first load the dataset with pd.read_csv(f, index_col = 1) then
        pass it directly into this function to clean it up and get it ready to go. 
        This function applies the symmetry rules to multiply the samples, removes uneeded
        columns, adjusts the drum angles to be in the correct cordinate system and scales
        quadrant powers to sum to 1. Arguments are included to skip expanding symmetries or
        to include the relative uncertainties reported by Serpent in the returned df.
2. careful_split : this function gives you your train and test split without making samples
        that come from the same calculation end up on other sides of the split. It is only
        needed because we use symmetry to expand our sample space. My logic as to why this is
        necessary is just that it is not a true split if results from the same calculation
        end up on separate sides of the split. I guess it is up to you to use it. However, I
        found that its use made debugging easier because with non-stochastic models
        all models will be completely symmetric because of the preserved symmetry of the
        training set

Some notes on the data:
    - responses are "fluxQ1", "fluxQ2", "fluxQ3" and "fluxQ4". Which are normalized to 1.
    - predictors are "theta1" - "theta8" which are drum angles in coordinate system described
          described by the README.md file. Here, they are bounded by [-pi, pi]
    - I included the serpent relative uncertainties if you do uncert=True in qpower_preprocess
          in case you wanted to do something with that. I think they are 2*the ccoefficient of
          variatin.

I have also included a little intro in majdi_ex.py to help get things started.

Once the model is done, we should have the ML model saved into a file so we can load it in without
needing to train a new model.

After this is done, the initialization routine in qpower_model needs to be filled out where self.eval is
assigned to the actual python function representing the ML model. It should take inputs as 8-element numpy
array representing drum rotations. Of course, if you think anything is better, no problem in doing that.

I also am regretting the file names of qpower_model.py and reactivity_model.py. We can change these once
we merge this branch into main

----------------------------------------------------------------------------------------------------------------------------------
Update from mir: 9/22/2021

1. baseline_ml.py: contains ML models of random forest, decision trees, linear regression, and neural networks.
                   use this script if you like to benchmark all models and to use final hyperparameters from tune_ml.py
2. tune_ml.py: contains a neural network tuning based on NEORL grid and random search tuners, this can be used to further make the NN better.
3. power_model.h5: this h5 NN model is generated by baseline_ml.py to be used in further analysis.
4. for_dean: this directory contains a final model and a script that shows how to make a prediction of sample using the pre-trained model.

results in this period were showing that LR is enough given strong linearity between cross-section and power, 
so neural network cannot acheive any better than baseline LR

RF Summary: MAE= 0.0013636160730347755 RMSE= 0.0017559282205638958 R2= 0.8385307867105468
DT Summary: MAE= 0.0024000427348890734 RMSE= 0.003066236084747103 R2= 0.5076347515063246
LR Summary: MAE= 0.000814437384479052 RMSE= 0.0010024383664269572 R2= 0.9473750650359852
NN Summary: MAE= 0.0007803090189752475 RMSE= 0.0010058203410773438 R2= 0.9470193794887553
------------------------------------------------------------------------------------------------------------------------------------
 
